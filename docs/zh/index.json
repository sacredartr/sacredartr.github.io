[{"content":"指标释义 单位净值: 基金单位（份额）净值，指每份额基金当日的价值，基金净资产除以基金总份额。 累计净值：基金最新净值与成立以来的分红业绩之和，需要考虑分红和拆分。 复权累计净值：分红+单位净值再投资进行复利计算。 累计收益率：常用统计区间为成立以来、今年以来，近三月，近六月，近一年，近两年，近三年。 计算方法：期末复权/期初复权再减一。 年化收益率：当前收益率（日收益率、周收益率、月收益率）换算成年收益率来计算的，是一种理论收益率。 计算方法：(期末复权/期初复权)^(365/日期差)再减一。 波动率: 波动率是指金融资产价格的波动程度。 计算方法为([(累计收益率-平均收益率)^2]的和/T-1)开根号 年化波动率：年化波动率把不同频率的波动率（日波动率、周波动率、月波动率）换算成年化波动率,是一种理论收益率。 计算方法：波动率*(计算周期开根号) 年化夏普比率：年化夏普比率是最常用的风险调整后收益指标，反映了承担每一单位风险而获得的超额收益率。 计算方法：(年化收益率-年化波动率)/无风险年化收益率 最大回撤：回撤是指在某一段时期内基金净值从最高点开始回落到最低点的变动幅度。 计算方法：(当前时期前最大累计净值-当前时期净值)/当前时期净值 年化卡玛比率：年化卡玛比率反映了承担每一单位风险而获得的超额收益率，采用最大回撤率来衡量风险，关注的是最极端的情况。 计算方法：(年化收益率-无风险年化收益率)/最大回撤 ","permalink":"https://sacredartr.github.io/zh/posts/fund-investment/","summary":"指标释义 单位净值: 基金单位（份额）净值，指每份额基金当日的价值，基金净资产除以基金总份额。 累计净值：基金最新净值与成立以来的分红业绩之和，需要考虑分红和拆分。 复权累计净值：分红+单位净值再投资进行复利计算。 累计收益率：常用统计区间为成立以来、今年以来，近三月，近六月，近一年，近两年，近三年。 计算方法：期末复权/期初复权再减一。 年化收益率：当前收益率（日收益率、周收益率、月收益率）换算成年收益率来计算的，是一种理论收益率。 计算方法：(期末复权/期初复权)^(365/日期差)再减一。 波动率: 波动率是指金融资产价格的波动程度。 计算方法为([(累计收益率-平均收益率)^2]的和/T-1)开根号 年化波动率：年化波动率把不同频率的波动率（日波动率、周波动率、月波动率）换算成年化波动率,是一种理论收益率。 计算方法：波动率*(计算周期开根号) 年化夏普比率：年化夏普比率是最常用的风险调整后收益指标，反映了承担每一单位风险而获得的超额收益率。 计算方法：(年化收益率-年化波动率)/无风险年化收益率 最大回撤：回撤是指在某一段时期内基金净值从最高点开始回落到最低点的变动幅度。 计算方法：(当前时期前最大累计净值-当前时期净值)/当前时期净值 年化卡玛比率：年化卡玛比率反映了承担每一单位风险而获得的超额收益率，采用最大回撤率来衡量风险，关注的是最极端的情况。 计算方法：(年化收益率-无风险年化收益率)/最大回撤 ","title":"Fund Investment"},{"content":"Kubernetes Daily Tips docker 添加代理 cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF mkdir -pv /etc/systemd/system/docker.service.d rm -rf /etc/systemd/system/docker.service.d/proxy.conf cat\u0026gt;/etc/systemd/system/docker.service.d/proxy.conf\u0026lt;\u0026lt;EOF [Service] EnvironmentFile=/etc/clash/env EOF systemctl daemon-reload systemctl restart docker containerd 添加代理 cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF sed -i \u0026#34;21i EnvironmentFile=/etc/clash/env\u0026#34; /etc/systemd/system/containerd.service systemctl daemon-reload systemctl restart containerd K8S 查看 pod 中 container 的 log kubectl get pods xxx -o jsonpath={.spec.containers[*].name} -n kube-system kubectl logs xxx -c xxxx -n kube-system ","permalink":"https://sacredartr.github.io/zh/posts/kubernetes-daily-tips/","summary":"Kubernetes Daily Tips docker 添加代理 cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF mkdir -pv /etc/systemd/system/docker.service.d rm -rf /etc/systemd/system/docker.service.d/proxy.conf cat\u0026gt;/etc/systemd/system/docker.service.d/proxy.conf\u0026lt;\u0026lt;EOF [Service] EnvironmentFile=/etc/clash/env EOF systemctl daemon-reload systemctl restart docker containerd 添加代理 cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1, 10.244.0.0/18\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1, 10.","title":"Kubernetes Daily Tips"},{"content":"词频统计 # encoding: utf-8 import jieba import pandas as pd import datetime as dt class wordCount(): def __init__(self): self.pre_data() def pre_data(self): jieba.load_userdict(\u0026#39;./dict.txt\u0026#39;) stopwords = pd.read_table(\u0026#39;./stop.txt\u0026#39;) self.stop_list = stopwords[\u0026#34;stop_word\u0026#34;].tolist() self.stop_list.append(\u0026#34;\\n\u0026#34;) def run(self, year, month): start = dt.date(year, month, 1) if month == 6: end = dt.date(year, month, 30) else: end = dt.date(year, month, 31) sql = \u0026#34;\u0026#34;\u0026#34;select fund_id, report_date, outlook as sentence from manager_viewpoint where report_date between \u0026#39;{}\u0026#39; and \u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format(dt.datetime.strftime(start, \u0026#39;%Y-%m-%d\u0026#39;), dt.datetime.strftime(end, \u0026#39;%Y-%m-%d\u0026#39;)) df = pd.DataFrame() df[\u0026#39;token\u0026#39;] = df[\u0026#39;sentence\u0026#39;].apply(lambda x: [i for i in list(jieba.cut(x)) if i not in self.stop_list]) words = [] for content in df[\u0026#39;token\u0026#39;]: words.extend(content) corpus = pd.DataFrame(words, columns=[\u0026#39;word\u0026#39;]) corpus[\u0026#39;count\u0026#39;] = 1 group = corpus.groupby(\u0026#39;word\u0026#39;)[\u0026#34;count\u0026#34;].count().sort_values(ascending=False).reset_index() group[\u0026#34;statistic_date\u0026#34;] = end print(group.tail(1)) if __name__ == \u0026#39;__main__\u0026#39;: word_count = wordCount() for year in [2021]: word_count.run(year, 6) ","permalink":"https://sacredartr.github.io/zh/posts/nlp/","summary":"词频统计 # encoding: utf-8 import jieba import pandas as pd import datetime as dt class wordCount(): def __init__(self): self.pre_data() def pre_data(self): jieba.load_userdict(\u0026#39;./dict.txt\u0026#39;) stopwords = pd.read_table(\u0026#39;./stop.txt\u0026#39;) self.stop_list = stopwords[\u0026#34;stop_word\u0026#34;].tolist() self.stop_list.append(\u0026#34;\\n\u0026#34;) def run(self, year, month): start = dt.date(year, month, 1) if month == 6: end = dt.date(year, month, 30) else: end = dt.date(year, month, 31) sql = \u0026#34;\u0026#34;\u0026#34;select fund_id, report_date, outlook as sentence from manager_viewpoint where report_date between \u0026#39;{}\u0026#39; and \u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format(dt.datetime.strftime(start, \u0026#39;%Y-%m-%d\u0026#39;), dt.","title":"NLP"},{"content":"Tools Daily Tips 艺术字 https://textkool.com/en/ascii-art-generator?hl=default\u0026amp;vl=default\u0026amp;font=Red%20Phoenix\u0026amp;text=Your%20text%20here%20 画图 https://www.draw.io/index.html k8s官方文档 https://kubernetes.io/zh-cn/ ","permalink":"https://sacredartr.github.io/zh/posts/tools-daily-tips/","summary":"Tools Daily Tips 艺术字 https://textkool.com/en/ascii-art-generator?hl=default\u0026amp;vl=default\u0026amp;font=Red%20Phoenix\u0026amp;text=Your%20text%20here%20 画图 https://www.draw.io/index.html k8s官方文档 https://kubernetes.io/zh-cn/ ","title":"Tools Daily Tips"},{"content":"Docker Daily Tips 清空所有容器 docker stop $(docker ps -q) \u0026amp;\u0026amp; docker rm $(docker ps -aq) docker 拉镜像:error response from daemon yum install bind-utils dig @114.114.114.114 registry-1.docker.io echo \u0026#34;54.175.43.85 registry-1.docker.io\u0026#34; \u0026gt;\u0026gt; /etc/hosts 清空指定容器 docker stop $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) \u0026amp;\u0026amp; docker rm $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) 创建 docker https registry mkdir -p /opt/docker/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/docker/registry/certs/domain.key -x509 -days 365 -out /opt/docker/registry/certs/domain.crt docker run -d --name registry2 -p 5000:5000 -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2 使用 docker https registry scp /opt/docker/registry/certs/domain.crt /etc/docker/certs.d/registry.docker.com:5000/ca.crt docker pull busybox:latest docker tag busybox registry.docker.com:5000/busybox:latest docker push registry.docker.com:5000/busybox:latest curl -X GET https://registry.docker.com:5000/v2/_catalog -k 创建随机镜像 FROM busybox:latest COPY test-data /test-data yum -y install buildah #!/bin bash tag=$1 for ((i=1;i\u0026lt;=100;i++)); do echo busybox:v$tag-$i dd if=/dev/random of=test-data bs=2M count=1 buildah bud --tag busybox:v$tag-$i rm -rf test-data buildah push localhost/busybox:v$tag-$i docker-daemon:busybox:v$tag-$i done ","permalink":"https://sacredartr.github.io/zh/posts/docker-daily-tips/","summary":"Docker Daily Tips 清空所有容器 docker stop $(docker ps -q) \u0026amp;\u0026amp; docker rm $(docker ps -aq) docker 拉镜像:error response from daemon yum install bind-utils dig @114.114.114.114 registry-1.docker.io echo \u0026#34;54.175.43.85 registry-1.docker.io\u0026#34; \u0026gt;\u0026gt; /etc/hosts 清空指定容器 docker stop $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) \u0026amp;\u0026amp; docker rm $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) 创建 docker https registry mkdir -p /opt/docker/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/docker/registry/certs/domain.key -x509 -days 365 -out /opt/docker/registry/certs/domain.crt docker run -d --name registry2 -p 5000:5000 -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.","title":"Docker Daily Tips"},{"content":"Git Daily Tips git 后续 提交 git commit --amend git 区分大小写 git config core.ignorecase false git mv -f old new git rebase git checkout mywork git rebase origin ","permalink":"https://sacredartr.github.io/zh/posts/git-daily-tips/","summary":"Git Daily Tips git 后续 提交 git commit --amend git 区分大小写 git config core.ignorecase false git mv -f old new git rebase git checkout mywork git rebase origin ","title":"Git Daily Tips"},{"content":"Linux Daily Tips 文件清空 cat /dev/null \u0026gt; file xargs ls ./* | xargs -i cp {} /tmp/ sed sed -i \u0026#39;s/password: .*/password: 123456/g\u0026#39; file awk cat file | awk \u0026#39;$1 ~ /th/ {print $1}\u0026#39; 执行命令忽略报错信息 unalias cp 2\u0026gt;/dev/null || true 文件追加内容 echo \u0026#39;content\u0026#39; \u0026gt;\u0026gt; file 登陆获取token token=$(curl -X POST \u0026#34;http://$external_ip/api/v1/login\u0026#34; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;,\u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;}\u0026#39; | awk -F\u0026#34;[,:}]\u0026#34; \u0026#39;{for(i=1;i\u0026lt;=NF;i++){print $(i+1)}}\u0026#39; | tr -d \u0026#39;\u0026#34;\u0026#39; | sed -n 1p) 执行命令时输入 echo yes | sh deploy.sh 文件尾部追加内容 cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF 文件任意位置追加内容 sed -i \u0026#34;21i EnvironmentFile=/etc/clash/env\u0026#34; /etc/systemd/system/containerd.service EOF ","permalink":"https://sacredartr.github.io/zh/posts/linux-daily-tips/","summary":"Linux Daily Tips 文件清空 cat /dev/null \u0026gt; file xargs ls ./* | xargs -i cp {} /tmp/ sed sed -i \u0026#39;s/password: .*/password: 123456/g\u0026#39; file awk cat file | awk \u0026#39;$1 ~ /th/ {print $1}\u0026#39; 执行命令忽略报错信息 unalias cp 2\u0026gt;/dev/null || true 文件追加内容 echo \u0026#39;content\u0026#39; \u0026gt;\u0026gt; file 登陆获取token token=$(curl -X POST \u0026#34;http://$external_ip/api/v1/login\u0026#34; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;,\u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;}\u0026#39; | awk -F\u0026#34;[,:}]\u0026#34; \u0026#39;{for(i=1;i\u0026lt;=NF;i++){print $(i+1)}}\u0026#39; | tr -d \u0026#39;\u0026#34;\u0026#39; | sed -n 1p) 执行命令时输入 echo yes | sh deploy.","title":"Linux Daily Tips"},{"content":"Mysql 数据库命令 获取所有表名 select `table_name` from information_schema.tables where `table_schema` = \u0026#39;{}\u0026#39; 获取所有列名 select `column_name` from information_schema.columns where `table_name`=\u0026#39;{}\u0026#39; and `table_schema` = \u0026#39;{}\u0026#39; *获取所有库名\nselect schema_name as `database` from information_schema.schemata *获取表中所有主键\nselect k.column_name from information_schema.table_constraints t join information_schema.key_column_usage k using (constraint_name,table_schema,table_name) where t.constraint_type=\u0026#39;PRIMARY KEY\u0026#39; and t.table_schema=\u0026#39;{}\u0026#39; and t.table_name=\u0026#39;{}\u0026#39; 创建视图 create view {} as select {} from {} 导出数据 mysqldump -uroot -p database table1 table2 \u0026gt; target.sql \u0026ndash;skip-opt 不锁表 -d 表示 只导出结构\n导入数据 mysql -uroot -p database \u0026lt; target.sql 备份恢复 xbstream -x -C /data \u0026lt; ./target.xb xtrabackup --decompress --target-dir=/data xtrabackup --prepare --target-dir=/data chown -R mysql:mysql /data mysqld_safe --defaults-file=/data/backup-my.cnf --user=mysql --datadir=/data \u0026amp; binlog mysqlbinlog --base64-output=decode-rows --verbose file | grep -i -A 100 delete dts import re import json import subprocess import pandas as pd from kafka import KafkaProducer from utils.db_config import base_backup from tencentcloud.common import credential from CKafka.Dts.dts_config import get_redis from CKafka.ckafka_config import bootstrap_servers from tencentcloud.common.profile.http_profile import HttpProfile from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException from tencentcloud.cdb.v20170320 import cdb_client, models class DataSyncDts(): def __init__(self): self.redis_client = get_redis() self.key = \u0026#34;dts_file_name\u0026#34; self.base_topic = \u0026#34;base.row_delete.topic\u0026#34; self.base_finance_new_topic = \u0026#34;base_finance_new.row_delete.topic\u0026#34; self.base_factor_topic = \u0026#34;base_finance_new.row_delete.topic\u0026#34; def init_data(self): print(\u0026#34;数据初始化...构建数据库字典\u0026#34;) self.database = {} sql = \u0026#34;\u0026#34;\u0026#34;select schema_name as `database` from information_schema.schemata\u0026#34;\u0026#34;\u0026#34; database = pd.read_sql(sql, base_backup()) for i, row in database.iterrows(): if row[\u0026#39;database\u0026#39;] not in [\u0026#34;base\u0026#34;, \u0026#34;base_finance_new\u0026#34;]: continue sql = \u0026#34;\u0026#34;\u0026#34;select `table_name` from information_schema.tables where `table_schema` = \u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format( row[\u0026#39;database\u0026#39;]) table = pd.read_sql(sql, base_backup()) table_column = {} for j, row_j in table.iterrows(): sql = \u0026#34;\u0026#34;\u0026#34;select k.column_name from information_schema.table_constraints t join information_schema.key_column_usage k using (constraint_name,table_schema,table_name) where t.constraint_type=\u0026#39;PRIMARY KEY\u0026#39; and t.table_schema=\u0026#39;{}\u0026#39; and t.table_name=\u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format( row[\u0026#34;database\u0026#34;], row_j[\u0026#34;table_name\u0026#34;]) key_column = pd.read_sql(sql, base_backup()) sql = \u0026#34;\u0026#34;\u0026#34;select `column_name` from information_schema.columns where `table_name`=\u0026#39;{}\u0026#39; and `table_schema` = \u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format( row_j[\u0026#34;table_name\u0026#34;], row[\u0026#34;database\u0026#34;]) column = pd.read_sql(sql, base_backup()) index_key_column = [] for k, row_k in key_column.iterrows(): index = column[\u0026#34;column_name\u0026#34;].values.tolist().index(row_k[\u0026#34;column_name\u0026#34;]) index_key_column.append((index, row_k[\u0026#34;column_name\u0026#34;])) index = None try: index = column[\u0026#34;column_name\u0026#34;].values.tolist().index(\u0026#39;update_time\u0026#39;) except Exception as e: pass if index: index_key_column.append((index, \u0026#39;update_time\u0026#39;)) table_column[row_j[\u0026#34;table_name\u0026#34;]] = index_key_column self.database[row[\u0026#34;database\u0026#34;]] = table_column def download(self): try: cred = credential.Credential(\u0026#34;******\u0026#34;, \u0026#34;******\u0026#34;) httpProfile = HttpProfile() httpProfile.endpoint = \u0026#34;cdb.tencentcloudapi.com\u0026#34; clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile client = cdb_client.CdbClient(cred, \u0026#34;ap-shanghai\u0026#34;, clientProfile) req = models.DescribeBinlogsRequest() params = { \u0026#34;InstanceId\u0026#34;: \u0026#34;cdb-2mvrb173\u0026#34; } req.from_json_string(json.dumps(params)) resp = client.DescribeBinlogs(req) returned = json.loads(resp.to_json_string()) url = returned[\u0026#39;Items\u0026#39;][0][\u0026#39;IntranetUrl\u0026#39;] file_name = returned[\u0026#39;Items\u0026#39;][0][\u0026#39;Name\u0026#39;] self.file_name = \u0026#34;/home/{}\u0026#34;.format(file_name) dts_file_name = self.redis_client.get(self.key).decode(\u0026#39;utf-8\u0026#39;) print(dts_file_name) if dts_file_name == file_name: base_backup().execute(\u0026#39;flush logs\u0026#39;) return False p = subprocess.Popen(\u0026#34;wget -c \u0026#39;{}\u0026#39; -O /home/{}\u0026#34;.format(url, file_name), shell=True, stdout=subprocess.PIPE) p.wait() self.redis_client.set(self.key, file_name) return True except TencentCloudSDKException as err: print(err) return False def parse(self): producer = KafkaProducer(bootstrap_servers=bootstrap_servers, max_block_ms=60 * 1000) status, ret = subprocess.getstatusoutput( \u0026#39;/usr/src/app/CKafka/Dts/mysqlbinlog --base64-output=decode-rows --verbose {} | grep -i -A 300 delete\u0026#39;.format(self.file_name)) res = ret.replace(\u0026#39;#\u0026#39;,\u0026#39;\u0026#39;) res = res.replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) row_sql = re.findall(\u0026#34;(DELETE\\sFROM.*?)(at\\s\\d+|DELETE)\u0026#34;, res) for row in row_sql: try: database = re.findall(\u0026#34;DELETE\\sFROM\\s(.*?)\\..*?\\sWHERE\u0026#34;, row[0]) table = re.findall(\u0026#34;DELETE\\sFROM\\s.*?\\.(.*?)\\sWHERE\u0026#34;, row[0]) value = re.findall(\u0026#34;@\\d+=(.*?)\\s\\s\u0026#34;, row[0]) last_value = re.findall(\u0026#34;@{}=(.*?)\\s$\u0026#34;.format(str(len(value) + 1)), row[0]) value += last_value database = database[0].replace(\u0026#34;`\u0026#34;,\u0026#34;\u0026#34;) table = table[0].replace(\u0026#34;`\u0026#34;,\u0026#34;\u0026#34;) res = {} if database in [\u0026#34;base\u0026#34;, \u0026#34;base_finance_new\u0026#34;]: for i, row in enumerate(self.database[database][table]): res[row[1].replace(\u0026#34;\\\u0026#39;\u0026#34;,\u0026#34;\u0026#34;)] = value[row[0]].replace(\u0026#34;\\\u0026#39;\u0026#34;,\u0026#34;\u0026#34;) res[\u0026#34;Operation\u0026#34;] = \u0026#34;delete\u0026#34; res[\u0026#34;DbName\u0026#34;] = database res[\u0026#34;TableName\u0026#34;] = table res = json.dumps(res) data = bytes(\u0026#39;{}\u0026#39;.format(res),\u0026#39;utf-8\u0026#39;) print(data) if database == \u0026#34;base\u0026#34;: producer.send(self.base_topic, value=data) elif database == \u0026#34;base_finance_new\u0026#34;: producer.send(self.base_finance_new_topic, value=data) producer.flush() except Exception as e: print(e) if __name__ == \u0026#39;__main__\u0026#39;: dts = DataSyncDts() if dts.download(): dts.init_data() dts.parse() else: print(\u0026#34;最新binlog已解析，正在刷新binlog...\u0026#34;) ","permalink":"https://sacredartr.github.io/zh/posts/mysql/","summary":"Mysql 数据库命令 获取所有表名 select `table_name` from information_schema.tables where `table_schema` = \u0026#39;{}\u0026#39; 获取所有列名 select `column_name` from information_schema.columns where `table_name`=\u0026#39;{}\u0026#39; and `table_schema` = \u0026#39;{}\u0026#39; *获取所有库名\nselect schema_name as `database` from information_schema.schemata *获取表中所有主键\nselect k.column_name from information_schema.table_constraints t join information_schema.key_column_usage k using (constraint_name,table_schema,table_name) where t.constraint_type=\u0026#39;PRIMARY KEY\u0026#39; and t.table_schema=\u0026#39;{}\u0026#39; and t.table_name=\u0026#39;{}\u0026#39; 创建视图 create view {} as select {} from {} 导出数据 mysqldump -uroot -p database table1 table2 \u0026gt; target.sql \u0026ndash;skip-opt 不锁表 -d 表示 只导出结构","title":"Mysql"},{"content":"Python pandas常用命令 字符串转日期 dt.datetime.strptime(x, \u0026#39;%Y-%m-%d\u0026#39;).date() pd.to_datetime(x, format=\u0026#39;%Y-%m-%d\u0026#39;) 日期转字符串 dt.datetime.strftime(x, \u0026#39;%Y-%m-%d\u0026#39;) 日期相加 date + dt.timedelta(days=-1) merge pd.merge(left,right,how=\u0026#39;left\u0026#39;,on=\u0026#39;\u0026#39;) apply df[\u0026#39;x\u0026#39;].apply(lambda x: float(x)) groupyby返回第N行 df.groupby(\u0026#34;columns\u0026#34;).nth(N) 写入excel writer = pd.ExcelWriter(\u0026#34;demo.xlsx\u0026#34;) res.to_excel(writer, sheet_name=\u0026#34;middle\u0026#34;, index=False, header=False) res_final.to_excel(writer, sheet_name=\u0026#34;final\u0026#34;, index=False, header=False) 读取excel pd.read_excel(\u0026#39;demo.xlsx\u0026#39;, sheet_name=\u0026#39;middle\u0026#39;) explode 将每行指定columns中的list转为列 df.explode(\u0026#34;dirty\u0026#34;, ignore_index=True) squeeze 将series压缩成一个值 row.squeeze(\u0026#34;columns\u0026#34;) nlargest 前N个 df.nlargest(5, \u0026#34;columns\u0026#34;) nsmallest 后N个 df.nsmallest(5, \u0026#34;columns\u0026#34;) clip 异常值监测 df.clip(50, 60) ffill前向填充 df[\u0026#39;nav\u0026#39;].ffill() bfill后向填充 df[\u0026#39;nav\u0026#39;].bfill() stack 列旋转成行\nunstack 行旋转成列\n行转列特殊实战 df.set_index([df.groupby(\u0026#39;fund_id\u0026#39;).cumcount(), \u0026#39;fund_id\u0026#39;])[column_name].unstack().T 只取数据下三角 df.mask(np.triu(np.ones(df.shape, dtype=np.bool_))) 数字转时间 time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, time.localtime(x)) time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, time.localtime(float(x/1000))) datetime.datetime.fromtimestamp(x).strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) es查询使用 模糊匹配 from elasticsearch import Elasticsearch es_server = Elasticsearch([{\u0026#39;host\u0026#39;: \u0026#39;172.0.0.1\u0026#39;, \u0026#39;port\u0026#39;: 31696}]) search_body = { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;name\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;fund_name\u0026#34;, \u0026#34;fund_full_name\u0026#34;] } }, } es_server.search(index=\u0026#39;private_fund_info_list3\u0026#39;, body=search_body) 精确匹配 from elasticsearch import Elasticsearch es_server = Elasticsearch([{\u0026#39;host\u0026#39;: \u0026#39;172.0.0.1\u0026#39;, \u0026#39;port\u0026#39;: 31696}]) search_body = { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;phrase_prefix\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;fund_name\u0026#34;, \u0026#34;fund_full_name\u0026#34;] } }, } es_server.search(index=\u0026#39;private_fund_info_list3\u0026#39;, body=search_body) and与or查询 from elasticsearch import Elasticsearch es_server = Elasticsearch([{\u0026#39;host\u0026#39;: \u0026#39;172.0.0.1\u0026#39;, \u0026#39;port\u0026#39;: 31696}]) search_body = { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [{ \u0026#34;match_phrase\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;a\u0026#34; } }], \u0026#34;should\u0026#34;: [{ \u0026#34;match_phrase\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;b\u0026#34; } }, { \u0026#34;match_phrase\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;c\u0026#34; } }], } }, } es_server.search(index=\u0026#39;private_fund_info_list3\u0026#39;, body=search_body) url = \u0026#39;http://172.0.0.1:31696/private_fund_info_list3/_search\u0026#39; body = { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;JR307094\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;phrase_prefix\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;fund_id\u0026#34;] } } } es_bulk from elasticsearch import Elasticsearch from elasticsearch import helpers import pandas as pd from sqlalchemy import create_engine INDEX_NAME = \u0026#34;test\u0026#34; actions = [] CHUNKSIZE = 5000 INDEX_MAPPING = { \u0026#34;properties\u0026#34;: { \u0026#39;fund_id\u0026#39;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;}, \u0026#39;fund_name\u0026#39;: {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;}, \u0026#39;fund_full_name\u0026#39;: {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;}, } } es = Elasticsearch( [\u0026#39;127.0.0.1\u0026#39;], port=9200 ) es.indices.create(index=INDEX_NAME, ignore=[400]) es.indices.put_mapping(index=INDEX_NAME, doc_type=\u0026#39;manager\u0026#39;, body=INDEX_MAPPING, ignore=[400]) print(\u0026#34;Connect to mysql...\u0026#34;) _engine = create_engine(\u0026#34;sm_data_calc03:N853OVcSIH06Zgm@172.0.0.1:9618\u0026#34;, \u0026#39;test\u0026#39;) df = pd.read_sql(\u0026#34;\u0026#34;\u0026#34;select * from table\u0026#34;\u0026#34;\u0026#34;, _engine) for ix, row in df.iterrows(): d = row.dropna().to_dict() action = { \u0026#39;_index\u0026#39;: INDEX_NAME, \u0026#39;_type\u0026#39;: \u0026#39;manager\u0026#39;, \u0026#39;_source\u0026#39;: d } actions.append(action) if ix % CHUNKSIZE == 0: helpers.bulk(es, actions) actions.clear() print(\u0026#39;sm_info_es \u0026gt;\u0026gt;\u0026gt; insert or update:{}, take_time:{:.2f}\u0026#39;.format(ix, time.time() - t2)) if len(actions): helpers.bulk(es, actions) ","permalink":"https://sacredartr.github.io/zh/posts/python/","summary":"Python pandas常用命令 字符串转日期 dt.datetime.strptime(x, \u0026#39;%Y-%m-%d\u0026#39;).date() pd.to_datetime(x, format=\u0026#39;%Y-%m-%d\u0026#39;) 日期转字符串 dt.datetime.strftime(x, \u0026#39;%Y-%m-%d\u0026#39;) 日期相加 date + dt.timedelta(days=-1) merge pd.merge(left,right,how=\u0026#39;left\u0026#39;,on=\u0026#39;\u0026#39;) apply df[\u0026#39;x\u0026#39;].apply(lambda x: float(x)) groupyby返回第N行 df.groupby(\u0026#34;columns\u0026#34;).nth(N) 写入excel writer = pd.ExcelWriter(\u0026#34;demo.xlsx\u0026#34;) res.to_excel(writer, sheet_name=\u0026#34;middle\u0026#34;, index=False, header=False) res_final.to_excel(writer, sheet_name=\u0026#34;final\u0026#34;, index=False, header=False) 读取excel pd.read_excel(\u0026#39;demo.xlsx\u0026#39;, sheet_name=\u0026#39;middle\u0026#39;) explode 将每行指定columns中的list转为列 df.explode(\u0026#34;dirty\u0026#34;, ignore_index=True) squeeze 将series压缩成一个值 row.squeeze(\u0026#34;columns\u0026#34;) nlargest 前N个 df.nlargest(5, \u0026#34;columns\u0026#34;) nsmallest 后N个 df.nsmallest(5, \u0026#34;columns\u0026#34;) clip 异常值监测 df.clip(50, 60) ffill前向填充 df[\u0026#39;nav\u0026#39;].ffill() bfill后向填充 df[\u0026#39;nav\u0026#39;].bfill() stack 列旋转成行\nunstack 行旋转成列\n行转列特殊实战 df.set_index([df.groupby(\u0026#39;fund_id\u0026#39;).cumcount(), \u0026#39;fund_id\u0026#39;])[column_name].unstack().T 只取数据下三角 df.mask(np.triu(np.ones(df.shape, dtype=np.bool_))) 数字转时间 time.","title":"Python"}]