[{"content":"bandit pip3 install bandit bandit -r project ","permalink":"https://sacredartr.github.io/posts/testing-tools/","summary":"bandit pip3 install bandit bandit -r project ","title":"Testing Tools"},{"content":"Helm Daily Tips helm deploy helm upgrade --install demo demo/ -n caas --values ./values.yaml --timeout 20m helm uninstall helm list -n caas helm uninstall demo -n caas helm debug helm dependency update helm upgrade --install demo demo/ -n caas --values ./values.yaml --timeout 20m --dry-run --debug helm package demo ","permalink":"https://sacredartr.github.io/posts/helm/","summary":"Helm Daily Tips helm deploy helm upgrade --install demo demo/ -n caas --values ./values.yaml --timeout 20m helm uninstall helm list -n caas helm uninstall demo -n caas helm debug helm dependency update helm upgrade --install demo demo/ -n caas --values ./values.yaml --timeout 20m --dry-run --debug helm package demo ","title":"helm"},{"content":"install keepalived yum install -y keepalived set keepalived.conf global_defs { router_id hostname # 标识本节点的字符串，设置为hostname即可 } # 定义脚本来检测服务健康状况 vrrp_script check_run { script \u0026#34;/etc/keepalived/scripts/check.sh\u0026#34; interval 5 # 每5秒检测一次 rise 3 # 连续成功3次才算成功 fall 2 # 连续失败2次才算失败 #weight -20 # 不配置权重值，检测失败就进入故障状态 } vrrp_instance VI_1 { state BACKUP # 节点状态，中心节点为 MASTER，协同节点为 BACKUP interface eth0 # VIP绑定的网卡接口 nopreempt # 设置非抢占模式 virtual_router_id 51 # 虚拟路由id，和备节点保持一致 priority 100 # 优先级，中心节点为 100，协同节点1 为 99，协同节点 2 为 98 mcast_src_ip 10.0.0.149 # 本机IP地址 advert_int 1 # MASTER和BACKUP节点之间的同步检查时间间隔，单位为秒 authentication { # 验证类型和验证密码 auth_type PASS # PAAS（默认），HA auth_pass 1111 # MASTER和BACKUP使用相同明文才可以互通 } # virtual_ipaddress { # vips # } track_script { #执行检测的脚本 check_run } # 切换到 MASTER 时启动服务，切换到其他状态都停止服务 notify_master /etc/keepalived/scripts/start-service.sh notify_backup /etc/keepalived/scripts/stop-service.sh notify_fault /etc/keepalived/scripts/stop-service.sh notify_stop /etc/keepalived/scripts/stop-service.sh } coredns apiVersion: apps/v1 kind: DaemonSet metadata: name: coredns spec: selector: matchLabels: app: coredns template: metadata: labels: app: coredns spec: hostNetwork: true # 使用本地网络 tolerations: # 这些容忍度设置是为了让该守护进程集在控制平面节点上运行 # 如果你不希望自己的控制平面节点运行 Pod，可以删除它们 - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: - name: coredns image: coredns/coredns:1.10.1 args: - -conf - /etc/coredns/Corefile resources: limits: memory: 170Mi requests: cpu: 10m memory: 70Mi volumeMounts: - name: conf mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP terminationGracePeriodSeconds: 30 volumes: - name: conf configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 data: Corefile: | .:53 { errors reload loadbalance hosts { 10.0.0.149 control.io # 每个节点分别配置成自己的 IP 10.0.0.149 navigate.io # 每个节点分别配置成自己的 IP 10.0.0.149 videomix.io # 每个节点分别配置成自己的 IP 10.0.0.149 wdadmin.io # 每个节点分别配置成自己的 IP fallthrough } forward . 114.114.114.114 } kind: ConfigMap metadata: name: coredns start-service #!/bin/bash set -x # 使用 kubectl 启动 coredns 以及调度服务 # 注意：需要手动指定 --kubeconfig 参数，keepalived 里执行不会去拿默认的 kubeconfig ( # 替换为当前集群内 kube-dns 的 clusterIP kubeDNS=10.96.0.10 # 删除 iptables 规则，允许外部访问本机 coredns # 因为 iptables 规则可以重复添加多次，因此 使用 while 循环，保证删除干净 rule_tcp=1 while [ \u0026#34;${rule_tcp}\u0026#34; == 1 ] do if iptables -C INPUT -p tcp --dport 53 ! -d ${kubeDNS} -j DROP then # 进入 then 表示退出码为 0，说明规则已经存在了，需要移除 echo \u0026#34;tcp规则存在\u0026#34; iptables -D INPUT -p tcp --dport 53 ! -d ${kubeDNS} -j DROP else # else 则表示退出码不为 0，规则不存在,不需要移除 echo \u0026#34;tcp规则已移除\u0026#34; rule_tcp=0 fi done rule_udp=1 while [ \u0026#34;${rule_udp}\u0026#34; == 1 ] do if iptables -C INPUT -p udp --dport 53 ! -d ${kubeDNS} -j DROP then # 进入 then 表示退出码为 0，说明规则已经存在了，需要移除 echo \u0026#34;tcp规则存在\u0026#34; iptables -D INPUT -p udp --dport 53 ! -d ${kubeDNS} -j DROP else # else 则表示退出码不为 0，规则不存在,不需要移除 echo \u0026#34;udp规则已移除\u0026#34; rule_udp=0 fi done basepath=\u0026#39;/etc/keepalived/deploy\u0026#39; schedulerNs=\u0026#39;default\u0026#39; # 启动业务 kubectl --kubeconfig /root/.kube/config -n ${schedulerNs} delete -f ${basepath}/deploy.yaml kubectl --kubeconfig /root/.kube/config -n ${schedulerNs} apply -f ${basepath}/deploy.yaml ) \u0026gt;\u0026gt;/tmp/start-service.log 2\u0026gt;\u0026amp;1 ","permalink":"https://sacredartr.github.io/posts/keepalived/","summary":"install keepalived yum install -y keepalived set keepalived.conf global_defs { router_id hostname # 标识本节点的字符串，设置为hostname即可 } # 定义脚本来检测服务健康状况 vrrp_script check_run { script \u0026#34;/etc/keepalived/scripts/check.sh\u0026#34; interval 5 # 每5秒检测一次 rise 3 # 连续成功3次才算成功 fall 2 # 连续失败2次才算失败 #weight -20 # 不配置权重值，检测失败就进入故障状态 } vrrp_instance VI_1 { state BACKUP # 节点状态，中心节点为 MASTER，协同节点为 BACKUP interface eth0 # VIP绑定的网卡接口 nopreempt # 设置非抢占模式 virtual_router_id 51 # 虚拟路由id，和备节点保持一致 priority 100 # 优先级，中心节点为 100，协同节点1 为 99，协同节点 2 为 98 mcast_src_ip 10.","title":"keepalived"},{"content":"Prow Deploy prepare kubernetes cluster github organization + app + webhook user kubectl create clusterrolebinding cluster-admin-binding-\u0026#34;${USER}\u0026#34; --clusterrole=cluster-admin --user=\u0026#34;${USER}\u0026#34; secret openssl rand -hex 20 \u0026gt; /path/to/hook/secret kubectl create secret -n prow generic hmac-token --from-file=hmac=/path/to/hook/secret kubectl create secret -n prow generic github-token --from-file=cert=/path/to/github/cert --from-literal=appid=\u0026lt;\u0026lt;The ID of your app\u0026gt;\u0026gt; deploy kubectl apply -f config-yaml/prow-deploy.yaml set config edit and install app at organization add webhook at github organization/pro update update-config: kubectl -n prow create configmap config --from-file=config.yaml=config.yaml --dry-run -o yaml | kubectl -n prow replace configmap config -f - update-plugins: kubectl -n prow create configmap plugins --from-file=plugins.yaml=plugins.yaml --dry-run -o yaml | kubectl -n prow replace configmap plugins -f - update-label-config: kubectl -n test-pods create configmap label-config --from-file=labels.yaml=labels.yaml --dry-run -o yaml | kubectl -n test-pods replace configmap label-config -f - ","permalink":"https://sacredartr.github.io/posts/prow-deploy/","summary":"Prow Deploy prepare kubernetes cluster github organization + app + webhook user kubectl create clusterrolebinding cluster-admin-binding-\u0026#34;${USER}\u0026#34; --clusterrole=cluster-admin --user=\u0026#34;${USER}\u0026#34; secret openssl rand -hex 20 \u0026gt; /path/to/hook/secret kubectl create secret -n prow generic hmac-token --from-file=hmac=/path/to/hook/secret kubectl create secret -n prow generic github-token --from-file=cert=/path/to/github/cert --from-literal=appid=\u0026lt;\u0026lt;The ID of your app\u0026gt;\u0026gt; deploy kubectl apply -f config-yaml/prow-deploy.yaml set config edit and install app at organization add webhook at github organization/pro update update-config: kubectl -n prow create configmap config --from-file=config.","title":"Prow Deploy"},{"content":"Harbor Deploy prepare kubernetes cluster helm crictl harbor.tar.xz redis.tar.xz pgo.tar.xz deploy pgo-ha # ctr -n k8s.io image import all pakage cd chart # created pgo namespace kubectl create ns pgo # install pgo controller CRD helm install -n pgo pgo . # check pod kubectl -n pgo get pod NAME READY STATUS RESTARTS AGE pgo-694f6b79bc-gpw2h 1/1 Running 0 16s pgo-694f6b79bc-lw9pl 1/1 Running 0 16s pgo-upgrade-76fdb74df8-ldzcr 1/1 Running 0 16s pgo-upgrade-76fdb74df8-zqrft 1/1 Running 0 16s # install ha-postgres kubectl create ns postgres kubectl -n postgres apply -f ha-postgres.yaml # check pod kubectl -n postgres get pod NAME READY STATUS RESTARTS AGE harbor-backup-c6tq-kzxd5 0/1 Completed 0 5m26s harbor-harbor-ha-instance-8ncb-0 4/4 Running 0 6m16s harbor-harbor-ha-instance-v8np-0 4/4 Running 0 6m16s harbor-pgbouncer-58d57f45d6-82skz 2/2 Running 0 6m15s harbor-pgbouncer-58d57f45d6-rks94 2/2 Running 0 6m15s harbor-repo-host-0 2/2 Running 0 6m16s # get secret kubectl -n postgres get secret harbor-pguser-harbor -o=jsonpath=\u0026#39;{@.data.password}\u0026#39; | base64 -d # get host kubectl -n postgres get secret harbor-pguser-harbor -o=jsonpath=\u0026#39;{@.data.host}\u0026#39; | base64 -d deploy redis-ha # ctr -n k8s.io image import all pakage cd chart # create redis namespace kubectl create ns redis # install redis helm install -n redis redis . # check pod kubectl -n redis get pod NAME READY STATUS RESTARTS AGE redis-redis-ha-server-0 3/3 Running 0 25s pgo-upgrade-76fdb74df8-zqrft 1/1 Running 0 16s # domain redis+sentienl redis-redis-ha.redis.svc.cluster.local k8s-haproxy redis-redis-ha-haproxy.redis.svc.cluster.local deploy harbor-ha # ctr -n k8s.io image import all pakage cd chart # create namespace kubectl create ns harbor # install harbor helm install -n harbor harbor . # check pod kubectl -n harbor get pod NAME READY STATUS RESTARTS AGE harbor-core-6cb79d76-zcjwn 1/1 Running 0 3m44s harbor-jobservice-6b8bd7d6d9-7247v 1/1 Running 0 3m44s harbor-nginx-7756ccb484-n7vpb 1/1 Running 0 3m44s harbor-portal-57cf48cfc8-7b6tq 1/1 Running 0 10m harbor-registry-cccdf8f69-m8cws 2/2 Running 0 3m44s ","permalink":"https://sacredartr.github.io/posts/harbor-deploy/","summary":"Harbor Deploy prepare kubernetes cluster helm crictl harbor.tar.xz redis.tar.xz pgo.tar.xz deploy pgo-ha # ctr -n k8s.io image import all pakage cd chart # created pgo namespace kubectl create ns pgo # install pgo controller CRD helm install -n pgo pgo . # check pod kubectl -n pgo get pod NAME READY STATUS RESTARTS AGE pgo-694f6b79bc-gpw2h 1/1 Running 0 16s pgo-694f6b79bc-lw9pl 1/1 Running 0 16s pgo-upgrade-76fdb74df8-ldzcr 1/1 Running 0 16s pgo-upgrade-76fdb74df8-zqrft 1/1 Running 0 16s # install ha-postgres kubectl create ns postgres kubectl -n postgres apply -f ha-postgres.","title":"Harbor Deploy"},{"content":"Ceph Deploy prepare 3 centos x86 |10.0.0.1|2C|4G|50G|mount 50G disk| |10.0.0.2|2C|4G|50G|mount 50G disk| |10.0.0.3|2C|4G|50G|mount50G disk| hostnamectl set-hostname node1 hostnamectl set-hostname node2 hostnamectl set-hostname node3 cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 10.0.0.1 node1 10.0.0.2 node2 10.0.0.3 node3 EOF install dependent packages on 3 machine cat \u0026gt; /etc/yum.repos.d/ceph.repo \u0026lt;\u0026lt;EOF [noarch] name=Ceph noarch baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ enabled=1 gpgcheck=0 [x86_64] name=Ceph x86_64 baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/ enabled=1 gpgcheck=0 EOF systemctl disable --now firewalld setenforce 0 sed -i \u0026#39;s/^SELINUX=.*/SELINUX=disabled/\u0026#39; /etc/selinux/config yum install -y chrony epel-release wget yum-utils systemctl enable --now chronyd yum install -y openssl-devel openssl-static zlib-devel lzma tk-devel xz-devel bzip2-devel ncurses-devel gdbm-devel readline-devel sqlite-devel gcc libffi-devel lvm2 install python on 3 machine wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz tar -xvf Python-3.7.0.tgz mv Python-3.7.0 /usr/local \u0026amp;\u0026amp; cd /usr/local/Python-3.7.0/ ./configure make make install ln -s /usr/local/Python-3.7.0/python /usr/bin/python3 cd yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum install docker-ce-19.03.9 -y mkdir /etc/docker echo \u0026#39;{\u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://hub-mirror.c.163.com\u0026#34;]}\u0026#39;\u0026gt;/etc/docker/daemon.json systemctl enable --now docker install ceph on node1 curl https://raw.githubusercontent.com/ceph/ceph/v15.2.1/src/cephadm/cephadm -o cephadm chmod +x cephadm ./cephadm add-repo --release octopus ./cephadm install which cephadm cephadm --help cephadm bootstrap --mon-ip 10.0.0.1 mkdir -p /etc/ceph touch /etc/ceph/ceph.conf alias ceph=\u0026#39;cephadm shell -- ceph\u0026#39; cephadm add-repo --release octopus cephadm install ceph-common ssh-copy-id -f -i /etc/ceph/ceph.pub root@node2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3 ceph orch host add node2 10.0.0.2 ceph orch host add node3 10.0.0.3 ceph orch host ls ceph orch host label add node1 mon ceph orch host label add node2 mon ceph orch host label add node3 mon ceph orch apply mon node1 ceph orch apply mon node2 ceph orch apply mon node3 wait for 20 minutes and continue(docker ps on node2 and node3 then deploy on node1) ceph orch daemon add osd node1:/dev/sdb ceph orch daemon add osd node2:/dev/sdb ceph orch daemon add osd node3:/dev/sdb ceph orch device ls ceph -s use ceph pool on k8s ceph osd pool create kubernetes 2 2 rbd pool init kubernetes ceph auth get-or-create client.kubernetes mon \u0026#39;profile rbd\u0026#39; osd \u0026#39;profile rbd pool=kubernetes\u0026#39; mgr \u0026#39;profile rbd pool=kubernetes\u0026#39; ceph mon dump parameter description kubernetes：pool name key: pool key fsid: ceph k8s id mon: k8s monitor ip(10.0.0.1:6789) ","permalink":"https://sacredartr.github.io/posts/ceph-deploy/","summary":"Ceph Deploy prepare 3 centos x86 |10.0.0.1|2C|4G|50G|mount 50G disk| |10.0.0.2|2C|4G|50G|mount 50G disk| |10.0.0.3|2C|4G|50G|mount50G disk| hostnamectl set-hostname node1 hostnamectl set-hostname node2 hostnamectl set-hostname node3 cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 10.0.0.1 node1 10.0.0.2 node2 10.0.0.3 node3 EOF install dependent packages on 3 machine cat \u0026gt; /etc/yum.repos.d/ceph.repo \u0026lt;\u0026lt;EOF [noarch] name=Ceph noarch baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ enabled=1 gpgcheck=0 [x86_64] name=Ceph x86_64 baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/ enabled=1 gpgcheck=0 EOF systemctl disable --now firewalld setenforce 0 sed -i \u0026#39;s/^SELINUX=.*/SELINUX=disabled/\u0026#39; /etc/selinux/config yum install -y chrony epel-release wget yum-utils systemctl enable --now chronyd yum install -y openssl-devel openssl-static zlib-devel lzma tk-devel xz-devel bzip2-devel ncurses-devel gdbm-devel readline-devel sqlite-devel gcc libffi-devel lvm2 install python on 3 machine wget https://www.","title":"Ceph Deploy"},{"content":"Kubernetes Daily Tips docker add proxy cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF mkdir -pv /etc/systemd/system/docker.service.d rm -rf /etc/systemd/system/docker.service.d/proxy.conf cat\u0026gt;/etc/systemd/system/docker.service.d/proxy.conf\u0026lt;\u0026lt;EOF [Service] EnvironmentFile=/etc/clash/env EOF systemctl daemon-reload systemctl restart docker containerd add proxy cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF sed -i \u0026#34;21i EnvironmentFile=/etc/clash/env\u0026#34; /etc/systemd/system/containerd.service systemctl daemon-reload systemctl restart containerd patch kubectl patch svc mariadb -n demo --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;:30006}]\u0026#39; kubectl patch svc rabbitmq -n demo --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;:30007},{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/1/nodePort\u0026#34;,\u0026#34;value\u0026#34;:30008},{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/2/nodePort\u0026#34;,\u0026#34;value\u0026#34;:30009},{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/3/nodePort\u0026#34;,\u0026#34;value\u0026#34;:30010}]\u0026#39; delete kubectl delete --all deploy -n demo kubectl delete --all statefulset -n demo kubectl delete --all pod -n demo kubectl delete --all pvc -n demo kubectl delete --all pv -n demo kubectl delete ns demo images crictl rmi docker.io/busybox:latest ctr -n k8s.io image export busybox.tar.gz docker.io/busybox:latest ctr -n k8s.io image import busybox.tar.gz ctr -n k8s.io image pull docker.io/busybox:latest rollout kubectl rollout restart deploy busybox namespace delete failed 1. kubect delete crd resource -n caas --force 2. kubectl edit crd resource -n caas remove finalizer 3. kubectl get namespace monitoring -o json \u0026gt; monitoring.json vi monitoring.json remove finalizer kubectl proxy curl -k -H \u0026#34;Content-Type: application/json\u0026#34; -X PUT --data-binary @monitoring.json http://127.0.0.1:8001/api/v1/namespaces/monitoring/finalize recover split-brain 1.【all node】ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key --endpoints=https://node1_ip:2379,https://node2_ip:2379,https://node3_ip:2379 endpoint status --write-out=json | jq find the nodes with a revision difference greater than 1000 2.【healthy node】ETCDCTL_API=3 etcdctl --endpoints=https://node_ip:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save healthy.bak 3.【all node】mv /etc/kubernetes/manifests/* ./manifests/ 4.【all node】rm -rf /var/lib/etcd/* 5.【all node】ETCDCTL_API=3 etcdctl --name ${noden_name} --initial-cluster ${node1_name}=https://${node1_ip}:2380,${node2_name}=https://${node2_ip}:2380,${node3_name}=https://${node3_ip}:2380 --initial-cluster-token etcd-cluster-1 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt --initial-advertise-peer-urls https://${noden_ip}:2380 snapshot restore healthy.bak --data-dir /var/lib/etcd 6.【healthy node】ETCDCTL_API=3 etcdctl --endpoints=https://${node1_ip}:2379,https://${node2_ip}:2379,https://${node3_ip}:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key endpoint health check etcd health 7.【all node】mv ./manifests/* /etc/kubernetes/manifests/ ","permalink":"https://sacredartr.github.io/posts/kubernetes-daily-tips/","summary":"Kubernetes Daily Tips docker add proxy cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF mkdir -pv /etc/systemd/system/docker.service.d rm -rf /etc/systemd/system/docker.service.d/proxy.conf cat\u0026gt;/etc/systemd/system/docker.service.d/proxy.conf\u0026lt;\u0026lt;EOF [Service] EnvironmentFile=/etc/clash/env EOF systemctl daemon-reload systemctl restart docker containerd add proxy cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export http_proxy=http://$IP:7890 export https_proxy=http://$IP:7890 export no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF source /etc/profile rm -rf /etc/clash/env mkdir -pv /etc/clash cat\u0026gt;/etc/clash/env\u0026lt;\u0026lt;EOF http_proxy=http://$IP:7890 https_proxy=http://$IP:7890 no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF sed -i \u0026#34;21i EnvironmentFile=/etc/clash/env\u0026#34; /etc/systemd/system/containerd.","title":"Kubernetes Daily Tips"},{"content":"Word frequency # encoding: utf-8 import jieba import pandas as pd import datetime as dt class wordCount(): def __init__(self): self.pre_data() def pre_data(self): jieba.load_userdict(\u0026#39;./dict.txt\u0026#39;) stopwords = pd.read_table(\u0026#39;./stop.txt\u0026#39;) self.stop_list = stopwords[\u0026#34;stop_word\u0026#34;].tolist() self.stop_list.append(\u0026#34;\\n\u0026#34;) def run(self, year, month): start = dt.date(year, month, 1) if month == 6: end = dt.date(year, month, 30) else: end = dt.date(year, month, 31) sql = \u0026#34;\u0026#34;\u0026#34;select fund_id, report_date, outlook as sentence from manager_viewpoint where report_date between \u0026#39;{}\u0026#39; and \u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format(dt.datetime.strftime(start, \u0026#39;%Y-%m-%d\u0026#39;), dt.datetime.strftime(end, \u0026#39;%Y-%m-%d\u0026#39;)) df = pd.DataFrame() df[\u0026#39;token\u0026#39;] = df[\u0026#39;sentence\u0026#39;].apply(lambda x: [i for i in list(jieba.cut(x)) if i not in self.stop_list]) words = [] for content in df[\u0026#39;token\u0026#39;]: words.extend(content) corpus = pd.DataFrame(words, columns=[\u0026#39;word\u0026#39;]) corpus[\u0026#39;count\u0026#39;] = 1 group = corpus.groupby(\u0026#39;word\u0026#39;)[\u0026#34;count\u0026#34;].count().sort_values(ascending=False).reset_index() group[\u0026#34;statistic_date\u0026#34;] = end print(group.tail(1)) if __name__ == \u0026#39;__main__\u0026#39;: word_count = wordCount() for year in [2021]: word_count.run(year, 6) ","permalink":"https://sacredartr.github.io/posts/nlp/","summary":"Word frequency # encoding: utf-8 import jieba import pandas as pd import datetime as dt class wordCount(): def __init__(self): self.pre_data() def pre_data(self): jieba.load_userdict(\u0026#39;./dict.txt\u0026#39;) stopwords = pd.read_table(\u0026#39;./stop.txt\u0026#39;) self.stop_list = stopwords[\u0026#34;stop_word\u0026#34;].tolist() self.stop_list.append(\u0026#34;\\n\u0026#34;) def run(self, year, month): start = dt.date(year, month, 1) if month == 6: end = dt.date(year, month, 30) else: end = dt.date(year, month, 31) sql = \u0026#34;\u0026#34;\u0026#34;select fund_id, report_date, outlook as sentence from manager_viewpoint where report_date between \u0026#39;{}\u0026#39; and \u0026#39;{}\u0026#39;\u0026#34;\u0026#34;\u0026#34;.format(dt.datetime.strftime(start, \u0026#39;%Y-%m-%d\u0026#39;), dt.","title":"NLP"},{"content":"Tools Daily Tips ASCII text art generator https://textkool.com/en/ascii-art-generator?hl=default\u0026amp;vl=default\u0026amp;font=Red%20Phoenix\u0026amp;text=Your%20text%20here%20 draw https://www.draw.io/index.html kubernetes https://kubernetes.io/ chart package https://artifacthub.io/ ","permalink":"https://sacredartr.github.io/posts/tools-daily-tips/","summary":"Tools Daily Tips ASCII text art generator https://textkool.com/en/ascii-art-generator?hl=default\u0026amp;vl=default\u0026amp;font=Red%20Phoenix\u0026amp;text=Your%20text%20here%20 draw https://www.draw.io/index.html kubernetes https://kubernetes.io/ chart package https://artifacthub.io/ ","title":"Tools Daily Tips"},{"content":"Docker Daily Tips clear all containers docker stop $(docker ps -q) \u0026amp;\u0026amp; docker rm $(docker ps -aq) docker pull:error response from daemon yum install bind-utils dig @114.114.114.114 registry-1.docker.io echo \u0026#34;54.175.43.85 registry-1.docker.io\u0026#34; \u0026gt;\u0026gt; /etc/hosts clear specified containers docker stop $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) \u0026amp;\u0026amp; docker rm $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) create docker https registry mkdir -p /opt/docker/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/docker/registry/certs/domain.key -x509 -days 365 -out /opt/docker/registry/certs/domain.crt docker run -d --name registry2 -p 5000:5000 -v /opt/docker/registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2 use docker https registry scp /opt/docker/registry/certs/domain.crt /etc/docker/certs.d/registry.docker.com:5000/ca.crt docker pull busybox:latest docker tag busybox registry.docker.com:5000/busybox:latest docker push registry.docker.com:5000/busybox:latest curl -X GET https://registry.docker.com:5000/v2/_catalog -k create random image FROM busybox:latest COPY test-data /test-data yum -y install buildah #!/bin bash tag=$1 for ((i=1;i\u0026lt;=100;i++)); do echo busybox:v$tag-$i dd if=/dev/random of=test-data bs=2M count=1 buildah bud --tag busybox:v$tag-$i rm -rf test-data buildah push localhost/busybox:v$tag-$i docker-daemon:busybox:v$tag-$i done ","permalink":"https://sacredartr.github.io/posts/docker-daily-tips/","summary":"Docker Daily Tips clear all containers docker stop $(docker ps -q) \u0026amp;\u0026amp; docker rm $(docker ps -aq) docker pull:error response from daemon yum install bind-utils dig @114.114.114.114 registry-1.docker.io echo \u0026#34;54.175.43.85 registry-1.docker.io\u0026#34; \u0026gt;\u0026gt; /etc/hosts clear specified containers docker stop $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) \u0026amp;\u0026amp; docker rm $(docker ps -a|grep hours|awk \u0026#39;{print $1}\u0026#39;) create docker https registry mkdir -p /opt/docker/registry/certs openssl req -newkey rsa:4096 -nodes -sha256 -keyout /opt/docker/registry/certs/domain.key -x509 -days 365 -out /opt/docker/registry/certs/domain.","title":"Docker Daily Tips"},{"content":"Git Daily Tips git subsequent commit git commit --amend git case sensitive git config core.ignorecase false git mv -f old new submodule git submodule add \u0026lt;submodule_url\u0026gt; git submodule init git submodule update git submodule update --remote ","permalink":"https://sacredartr.github.io/posts/git-daily-tips/","summary":"Git Daily Tips git subsequent commit git commit --amend git case sensitive git config core.ignorecase false git mv -f old new submodule git submodule add \u0026lt;submodule_url\u0026gt; git submodule init git submodule update git submodule update --remote ","title":"Git Daily Tips"},{"content":"Linux Daily Tips clear file cat /dev/null \u0026gt; file xargs ls ./* | xargs -i cp {} /tmp/ sed sed -i \u0026#39;s/password: .*/password: 123456/g\u0026#39; file awk cat file | awk \u0026#39;$1 ~ /th/ {print $1}\u0026#39; ignore error message unalias cp 2\u0026gt;/dev/null || true append content to file echo \u0026#39;content\u0026#39; \u0026gt;\u0026gt; file login to get token token=$(curl -X POST \u0026#34;http://$external_ip/api/v1/login\u0026#34; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;,\u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;}\u0026#39; | awk -F\u0026#34;[,:}]\u0026#34; \u0026#39;{for(i=1;i\u0026lt;=NF;i++){print $(i+1)}}\u0026#39; | tr -d \u0026#39;\u0026#34;\u0026#39; | sed -n 1p) input when executing command echo yes | sh deploy.sh append content to end of file cat\u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt;EOF export no_proxy=\u0026#34;localhost, 127.0.0.1\u0026#34; EOF append content to anywhere in the file sed -i \u0026#34;21i EnvironmentFile=/etc/clash/env\u0026#34; /etc/systemd/system/containerd.service execute shell on ssh command ssh -i ~/.ssh/key root@${HOST_IP} \u0026#39;bash -s\u0026#39; \u0026lt;\u0026lt; \u0026#39;END\u0026#39; kubectl patch svc demo -n demo --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/type\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;NodePort\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;add\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/spec/ports/0/nodePort\u0026#34;,\u0026#34;value\u0026#34;:30011}]\u0026#39; END delete matching directory find /tmp/ -maxdepth 1 -name \u0026#39;yarn*\u0026#39; | xargs rm -rf configure automatic disk mounting # find UUID blkid /dev/vdb1 vi /etc/fstab UUID=a7dd2c70-132e-446e-a0f8-41d6547445fb /data ext4 defaults 0 0 ","permalink":"https://sacredartr.github.io/posts/linux-daily-tips/","summary":"Linux Daily Tips clear file cat /dev/null \u0026gt; file xargs ls ./* | xargs -i cp {} /tmp/ sed sed -i \u0026#39;s/password: .*/password: 123456/g\u0026#39; file awk cat file | awk \u0026#39;$1 ~ /th/ {print $1}\u0026#39; ignore error message unalias cp 2\u0026gt;/dev/null || true append content to file echo \u0026#39;content\u0026#39; \u0026gt;\u0026gt; file login to get token token=$(curl -X POST \u0026#34;http://$external_ip/api/v1/login\u0026#34; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;,\u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;}\u0026#39; | awk -F\u0026#34;[,:}]\u0026#34; \u0026#39;{for(i=1;i\u0026lt;=NF;i++){print $(i+1)}}\u0026#39; | tr -d \u0026#39;\u0026#34;\u0026#39; | sed -n 1p) input when executing command echo yes | sh deploy.","title":"Linux Daily Tips"}]